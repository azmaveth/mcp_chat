# MCP Chat Configuration Example
# 
# This is a comprehensive example configuration file for MCP Chat.
# Copy this to ~/.config/mcp_chat/config.toml and customize as needed.
#
# Environment variables can override config values:
# - ANTHROPIC_API_KEY
# - OPENAI_API_KEY

# ==============================================================================
# LLM Configuration
# ==============================================================================

[llm]
# Default LLM backend to use (anthropic, openai, local)
default = "anthropic"

# ------------------------------------------------------------------------------
# Anthropic Claude Configuration
# ------------------------------------------------------------------------------
[llm.anthropic]
# API key - leave empty to use ANTHROPIC_API_KEY environment variable
api_key = ""

# Model to use - available models:
# - claude-sonnet-4-20250514 (recommended, latest)
# - claude-opus-4-20250514 (most capable)
# - claude-3-5-sonnet-20241022
# - claude-3-5-haiku-20241022 (fastest)
# - claude-3-opus-20240229
# - claude-3-sonnet-20240229
# - claude-3-haiku-20240307
model = "claude-sonnet-4-20250514"

# Maximum tokens for response
max_tokens = 4096

# Temperature (0.0-1.0, higher = more creative)
temperature = 0.7

# System prompt (optional)
# system_prompt = "You are a helpful assistant."

# ------------------------------------------------------------------------------
# OpenAI Configuration
# ------------------------------------------------------------------------------
[llm.openai]
# API key - leave empty to use OPENAI_API_KEY environment variable
api_key = ""

# Model to use - available models:
# - gpt-4-turbo (latest GPT-4)
# - gpt-4-turbo-preview
# - gpt-4
# - gpt-3.5-turbo (faster, cheaper)
model = "gpt-4-turbo"

# Maximum tokens for response
max_tokens = 4096

# Temperature (0.0-2.0, higher = more creative)
temperature = 0.7

# Top-p sampling (0.0-1.0)
top_p = 1.0

# Frequency penalty (-2.0-2.0)
frequency_penalty = 0.0

# Presence penalty (-2.0-2.0)
presence_penalty = 0.0

# ------------------------------------------------------------------------------
# Local Model Configuration (Bumblebee/Nx)
# ------------------------------------------------------------------------------
[llm.local]
# Path to model files
model_path = "models/llama-2-7b"

# Device to use (cpu, cuda, metal)
device = "cpu"

# Model type (llama, mistral, etc.)
model_type = "llama"

# Maximum tokens
max_tokens = 2048

# Temperature
temperature = 0.7

# ==============================================================================
# MCP Server Configuration
# ==============================================================================

[mcp]
# List of MCP servers to connect to

# ------------------------------------------------------------------------------
# Filesystem Server - Access local files
# ------------------------------------------------------------------------------
[[mcp.servers]]
name = "filesystem"
description = "Access local filesystem"
command = ["npx", "-y", "@modelcontextprotocol/server-filesystem", "$HOME"]
# Alternative with multiple directories:
# command = ["npx", "-y", "@modelcontextprotocol/server-filesystem", "/home/user", "/tmp", "/var/log"]

# ------------------------------------------------------------------------------
# GitHub Server - Access GitHub repositories
# ------------------------------------------------------------------------------
[[mcp.servers]]
name = "github"
description = "Access GitHub repositories"
command = ["npx", "-y", "@modelcontextprotocol/server-github"]
env = { GITHUB_TOKEN = "" }  # Add your GitHub personal access token

# ------------------------------------------------------------------------------
# PostgreSQL Server - Database access
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "postgres"
# description = "PostgreSQL database"
# command = ["npx", "-y", "@modelcontextprotocol/server-postgres"]
# env = { DATABASE_URL = "postgresql://user:password@localhost:5432/mydb" }

# ------------------------------------------------------------------------------
# Remote SSE Server - Connect to remote MCP server
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "remote-assistant"
# description = "Remote AI assistant"
# transport = "sse"
# url = "http://api.example.com:8080/sse"
# headers = { "Authorization" = "Bearer token123" }

# ------------------------------------------------------------------------------
# Custom Local Server
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "my-tools"
# description = "Custom tools server"
# command = ["python", "/path/to/my-server.py"]
# working_dir = "/path/to/project"
# env = { 
#   CUSTOM_VAR = "value",
#   DEBUG = "true"
# }

# ==============================================================================
# MCP Server Mode Configuration
# ==============================================================================

[mcp_server]
# Enable stdio server mode (for other MCP clients to connect via stdio)
stdio_enabled = false

# Enable SSE server mode (for other MCP clients to connect via HTTP/SSE)
sse_enabled = false

# Port for SSE server
sse_port = 8080

# Host to bind SSE server (use 0.0.0.0 for all interfaces)
sse_host = "127.0.0.1"

# ==============================================================================
# UI Configuration
# ==============================================================================

[ui]
# Color theme (dark, light, auto)
theme = "dark"

# Number of messages to keep in history
history_size = 1000

# Enable/disable Unicode emoji
emoji = true

# Terminal width override (leave empty for auto-detect)
# width = 120

# Show timestamps on messages
show_timestamps = true

# Date format for timestamps
timestamp_format = "%H:%M:%S"

# ==============================================================================
# Context Management
# ==============================================================================

[context]
# Default maximum tokens for context window
max_tokens = 4096

# Reserved tokens for response
reserve_tokens = 500

# Default truncation strategy (sliding_window, smart)
strategy = "sliding_window"

# System prompt to always include
# system_prompt = "You are a helpful AI assistant."

# ==============================================================================
# Session Management
# ==============================================================================

[session]
# Auto-save interval in seconds (0 to disable)
autosave_interval = 300

# Directory for saved sessions
save_directory = "~/.config/mcp_chat/sessions"

# Maximum number of saved sessions to keep
max_saved_sessions = 100

# ==============================================================================
# Logging and Debugging
# ==============================================================================

[debug]
# Log level (debug, info, warn, error)
log_level = "info"

# Log file path (empty to disable file logging)
log_file = "~/.config/mcp_chat/logs/mcp_chat.log"

# Enable debug mode for MCP protocol
mcp_debug = false

# ==============================================================================
# Advanced Settings
# ==============================================================================

[advanced]
# Request timeout in seconds
request_timeout = 60

# Connection retry attempts
retry_attempts = 3

# Retry delay in seconds
retry_delay = 1

# Maximum concurrent MCP server connections
max_connections = 10

# Enable experimental features
experimental = false