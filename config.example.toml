# MCP Chat Configuration Example
# 
# This is a comprehensive example configuration file for MCP Chat.
# Copy this to ~/.config/mcp_chat/config.toml and customize as needed.
#
# Environment variables can override config values:
# - ANTHROPIC_API_KEY
# - OPENAI_API_KEY
# - OLLAMA_API_BASE
# - GITHUB_TOKEN

# ==============================================================================
# LLM Configuration
# ==============================================================================

[llm]
# Default LLM backend to use (anthropic, openai, ollama, local, bedrock, gemini)
default = "anthropic"

# ------------------------------------------------------------------------------
# Anthropic Claude Configuration
# ------------------------------------------------------------------------------
[llm.anthropic]
# API key - leave empty to use ANTHROPIC_API_KEY environment variable
api_key = ""

# Model to use - available models:
# - claude-sonnet-4-20250514 (recommended, latest)
# - claude-opus-4-20250514 (most capable)
# - claude-3-5-sonnet-20241022
# - claude-3-5-haiku-20241022 (fastest)
# - claude-3-opus-20240229
# - claude-3-sonnet-20240229
# - claude-3-haiku-20240307
model = "claude-sonnet-4-20250514"

# Maximum tokens for response
max_tokens = 4096

# Temperature (0.0-1.0, higher = more creative)
temperature = 0.7

# Custom API endpoint (optional) - can also use ANTHROPIC_API_BASE env var
# api_base = "https://api.anthropic.com"

# System prompt (optional)
# system_prompt = "You are a helpful assistant."

# ------------------------------------------------------------------------------
# OpenAI Configuration
# ------------------------------------------------------------------------------
[llm.openai]
# API key - leave empty to use OPENAI_API_KEY environment variable
api_key = ""

# Model to use - available models:
# - gpt-4-turbo (latest GPT-4)
# - gpt-4-turbo-preview
# - gpt-4
# - gpt-3.5-turbo (faster, cheaper)
model = "gpt-4-turbo"

# Maximum tokens for response
max_tokens = 4096

# Temperature (0.0-2.0, higher = more creative)
temperature = 0.7

# Custom API endpoint (optional) - can also use OPENAI_API_BASE env var
# api_base = "https://api.openai.com/v1"

# Examples of OpenAI-compatible providers:
# - Venice.ai: api_base = "https://api.venice.ai/v1"
# - OpenRouter: api_base = "https://openrouter.ai/api/v1"
# - Groq: api_base = "https://api.groq.com/openai/v1"
# - Together.ai: api_base = "https://api.together.xyz/v1"
# - Azure OpenAI: api_base = "https://YOUR-RESOURCE.openai.azure.com"

# Top-p sampling (0.0-1.0)
top_p = 1.0

# Frequency penalty (-2.0-2.0)
frequency_penalty = 0.0

# Presence penalty (-2.0-2.0)
presence_penalty = 0.0

# ------------------------------------------------------------------------------
# Ollama Configuration (Local server)
# ------------------------------------------------------------------------------
[llm.ollama]
# Model to use - must be pulled with 'ollama pull <model>'
# Available models: llama2, mistral, codellama, phi, neural-chat, etc.
model = "llama2"

# API endpoint - can also use OLLAMA_API_BASE env var
# api_base = "http://localhost:11434"

# Temperature
temperature = 0.7

# ------------------------------------------------------------------------------
# Local Model Configuration (Bumblebee/EMLX)
# ------------------------------------------------------------------------------
[llm.local]
# Model to use - HuggingFace model ID or local path
# Examples:
# - "microsoft/phi-2"
# - "meta-llama/Llama-2-7b-hf"
# - "mistralai/Mistral-7B-v0.1"
# - "/path/to/local/model"
model_path = "microsoft/phi-2"

# Device to use (auto-detected if not specified)
# Options: cpu, cuda, metal
# device = "cpu"

# Maximum tokens
max_tokens = 2048

# Temperature
temperature = 0.7

# ------------------------------------------------------------------------------
# AWS Bedrock Configuration
# ------------------------------------------------------------------------------
[llm.bedrock]
# AWS credentials - leave empty to use AWS_ACCESS_KEY_ID/AWS_SECRET_ACCESS_KEY env vars
# access_key_id = ""
# secret_access_key = ""
# session_token = ""  # Optional, for temporary credentials

# AWS region - can also use AWS_REGION env var
region = "us-east-1"

# AWS profile to use (optional) - can also use AWS_PROFILE env var
# profile = "default"

# Model to use - available models:
# Anthropic:
# - claude-instant-v1
# - claude-v2
# - claude-3-sonnet
# - claude-3-haiku
# Amazon:
# - titan-lite
# - titan-express
# Meta:
# - llama2-13b
# - llama2-70b
# Cohere:
# - command
# - command-light
# Mistral:
# - mistral-7b
# - mixtral-8x7b
model = "claude-3-sonnet"

# Maximum tokens
max_tokens = 4096

# Temperature
temperature = 0.7

# ------------------------------------------------------------------------------
# Google Gemini Configuration
# ------------------------------------------------------------------------------
[llm.gemini]
# API key - leave empty to use GOOGLE_API_KEY environment variable
api_key = ""

# Model to use - available models:
# - gemini-pro (text only)
# - gemini-pro-vision (multimodal - text and images)
# - gemini-ultra (most capable, when available)
# - gemini-nano (lightweight, for edge devices)
model = "gemini-pro"

# Maximum tokens
max_tokens = 2048

# Temperature (0.0-1.0)
temperature = 0.7

# Top-p sampling (0.0-1.0)
top_p = 0.95

# Top-k sampling
top_k = 40

# Safety settings (optional) - threshold can be:
# - BLOCK_NONE
# - BLOCK_ONLY_HIGH
# - BLOCK_MEDIUM_AND_ABOVE (default)
# - BLOCK_LOW_AND_ABOVE
# [llm.gemini.safety_settings]
# HARM_CATEGORY_HARASSMENT = "BLOCK_MEDIUM_AND_ABOVE"
# HARM_CATEGORY_HATE_SPEECH = "BLOCK_MEDIUM_AND_ABOVE"
# HARM_CATEGORY_SEXUALLY_EXPLICIT = "BLOCK_MEDIUM_AND_ABOVE"
# HARM_CATEGORY_DANGEROUS_CONTENT = "BLOCK_MEDIUM_AND_ABOVE"

# ==============================================================================
# MCP Client Configuration
# ==============================================================================

[mcp]
# List of MCP servers to connect to

# ------------------------------------------------------------------------------
# Filesystem Server - Access local files
# ------------------------------------------------------------------------------
[[mcp.servers]]
name = "filesystem"
description = "Access local filesystem"
command = ["npx", "-y", "@modelcontextprotocol/server-filesystem", "$HOME"]
# Alternative with multiple directories:
# command = ["npx", "-y", "@modelcontextprotocol/server-filesystem", "/home/user", "/tmp", "/var/log"]

# Auto-connect on startup
auto_connect = true

# ------------------------------------------------------------------------------
# GitHub Server - Access GitHub repositories
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "github"
# description = "Access GitHub repositories"
# command = ["npx", "-y", "@modelcontextprotocol/server-github"]
# env = { GITHUB_TOKEN = "" }  # Or use GITHUB_TOKEN env var
# auto_connect = false

# ------------------------------------------------------------------------------
# PostgreSQL Server - Database access
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "postgres"
# description = "PostgreSQL database"
# command = ["npx", "-y", "@modelcontextprotocol/server-postgres"]
# env = { DATABASE_URL = "postgresql://user:password@localhost:5432/mydb" }

# ------------------------------------------------------------------------------
# SQLite Server - Local database access
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "sqlite"
# description = "SQLite database"
# command = ["npx", "-y", "@modelcontextprotocol/server-sqlite", "path/to/database.db"]

# ------------------------------------------------------------------------------
# Puppeteer Server - Browser automation
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "puppeteer"
# description = "Browser automation"
# command = ["npx", "-y", "@modelcontextprotocol/server-puppeteer"]

# ------------------------------------------------------------------------------
# Remote SSE Server - Connect to remote MCP server
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "remote-assistant"
# description = "Remote AI assistant"
# url = "http://api.example.com:8080"  # SSE endpoint
# # headers = { "Authorization" = "Bearer token123" }  # Optional auth

# ------------------------------------------------------------------------------
# Custom Local Server
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "my-tools"
# description = "Custom tools server"
# command = ["python", "/path/to/my-server.py"]
# working_dir = "/path/to/project"
# env = { 
#   CUSTOM_VAR = "value",
#   DEBUG = "true"
# }

# ==============================================================================
# Streaming Configuration
# ==============================================================================

[streaming]
# Use enhanced streaming with buffering and backpressure (default: true)
enhanced = true

# Buffer capacity - maximum chunks to buffer before applying backpressure
buffer_capacity = 100

# Write interval in milliseconds - how often to flush buffered chunks
write_interval = 25

# Minimum chunks to batch together before writing
min_batch_size = 3

# Maximum chunks per write operation
max_batch_size = 10

# MCP Server Mode Configuration
# ==============================================================================

[mcp_server]
# Enable stdio server mode (for other MCP clients to connect via stdio)
stdio_enabled = false

# Enable SSE server mode (for other MCP clients to connect via HTTP/SSE)
sse_enabled = false

# Port for SSE server
sse_port = 8080

# Host to bind SSE server (use 0.0.0.0 for all interfaces)
# sse_host = "127.0.0.1"

# ==============================================================================
# UI Configuration
# ==============================================================================

[ui]
# Color theme (dark, light, auto)
theme = "dark"

# Number of messages to keep in history
history_size = 1000

# Enable/disable Unicode emoji
emoji = true

# Enable streaming responses
streaming = true

# Show token counts
show_tokens = true

# Show cost estimates
show_cost = true

# Show timestamps on messages
show_timestamps = false

# Date format for timestamps
timestamp_format = "%H:%M:%S"

# ==============================================================================
# Context Management
# ==============================================================================

[context]
# Default maximum tokens for context window
max_tokens = 4096

# Reserved tokens for response
reserve_tokens = 500

# Default truncation strategy (sliding_window, smart)
strategy = "sliding_window"

# System prompt to always include
# system_prompt = "You are a helpful AI assistant."

# ==============================================================================
# Session Management
# ==============================================================================

[session]
# Auto-save sessions
auto_save = true

# Directory for saved sessions
save_directory = "~/.config/mcp_chat/sessions"

# Export format (markdown, json)
export_format = "markdown"

# ==============================================================================
# Command Aliases
# ==============================================================================

[aliases]
# Define custom command shortcuts
# status = ["/context", "/cost"]
# setup = ["/system You are a helpful assistant", "/tokens 8192"]
# morning = ["/clear", "Good morning! How can I help you today?"]

# ==============================================================================
# Logging and Debugging
# ==============================================================================

[debug]
# Log level (debug, info, warn, error)
log_level = "info"

# Log file path (empty to disable file logging)
# log_file = "~/.config/mcp_chat/logs/mcp_chat.log"

# Enable debug mode for MCP protocol
mcp_debug = false

# ==============================================================================
# Startup and Performance Settings
# ==============================================================================

[startup]
# MCP server connection mode:
# - "eager" = Connect to all servers at startup (fastest first use, slower startup)
# - "lazy" = Connect only when first needed (faster startup, slight delay on first use)
# - "background" = Start connections after UI loads (balanced approach)
mcp_connection_mode = "lazy"

# Enable startup time profiling (set via MCP_CHAT_STARTUP_PROFILING=true env var)
# profiling_enabled = false

# Parallel connection settings
[startup.parallel]
# Maximum number of concurrent server connections
max_concurrency = 4

# Connection timeout in milliseconds
connection_timeout = 10000

# Enable progress reporting during parallel connections
show_progress = true

# ==============================================================================
# Memory Management Settings
# ==============================================================================

[memory]
# Number of recent messages to keep in memory for quick access
session_cache_size = 20

# Messages per page when loading history
page_size = 50

# Maximum messages to store on disk per session
max_disk_size = 10000

# Memory limit for in-memory message cache
memory_limit = 100

# ==============================================================================
# Advanced Settings
# ==============================================================================

[advanced]
# Request timeout in seconds
request_timeout = 60

# Connection retry attempts
retry_attempts = 3

# Retry delay in seconds
retry_delay = 1

# Maximum concurrent MCP server connections
max_connections = 10

# ==============================================================================
# Resource Cache Configuration
# ==============================================================================

[resource_cache]
# Enable local caching of MCP resources
enabled = true

# Maximum cache size in bytes (100MB default)
max_size = 104857600

# Time to live for cached resources in seconds (1 hour default)
ttl = 3600

# Directory for storing cached resources
cache_directory = "~/.config/mcp_chat/resource_cache"

# Cleanup interval in seconds (5 minutes default)
cleanup_interval = 300