# MCP Chat Configuration Example
# 
# This is a comprehensive example configuration file for MCP Chat.
# Copy this to ~/.config/mcp_chat/config.toml and customize as needed.
#
# Environment variables can override config values:
# - ANTHROPIC_API_KEY
# - OPENAI_API_KEY
# - OLLAMA_API_BASE
# - GITHUB_TOKEN

# ==============================================================================
# LLM Configuration
# ==============================================================================

[llm]
# Default LLM backend to use (anthropic, openai, ollama, local)
default = "anthropic"

# ------------------------------------------------------------------------------
# Anthropic Claude Configuration
# ------------------------------------------------------------------------------
[llm.anthropic]
# API key - leave empty to use ANTHROPIC_API_KEY environment variable
api_key = ""

# Model to use - available models:
# - claude-sonnet-4-20250514 (recommended, latest)
# - claude-opus-4-20250514 (most capable)
# - claude-3-5-sonnet-20241022
# - claude-3-5-haiku-20241022 (fastest)
# - claude-3-opus-20240229
# - claude-3-sonnet-20240229
# - claude-3-haiku-20240307
model = "claude-sonnet-4-20250514"

# Maximum tokens for response
max_tokens = 4096

# Temperature (0.0-1.0, higher = more creative)
temperature = 0.7

# Custom API endpoint (optional) - can also use ANTHROPIC_API_BASE env var
# api_base = "https://api.anthropic.com"

# System prompt (optional)
# system_prompt = "You are a helpful assistant."

# ------------------------------------------------------------------------------
# OpenAI Configuration
# ------------------------------------------------------------------------------
[llm.openai]
# API key - leave empty to use OPENAI_API_KEY environment variable
api_key = ""

# Model to use - available models:
# - gpt-4-turbo (latest GPT-4)
# - gpt-4-turbo-preview
# - gpt-4
# - gpt-3.5-turbo (faster, cheaper)
model = "gpt-4-turbo"

# Maximum tokens for response
max_tokens = 4096

# Temperature (0.0-2.0, higher = more creative)
temperature = 0.7

# Custom API endpoint (optional) - can also use OPENAI_API_BASE env var
# api_base = "https://api.openai.com/v1"

# Examples of OpenAI-compatible providers:
# - Venice.ai: api_base = "https://api.venice.ai/v1"
# - OpenRouter: api_base = "https://openrouter.ai/api/v1"
# - Groq: api_base = "https://api.groq.com/openai/v1"
# - Together.ai: api_base = "https://api.together.xyz/v1"
# - Azure OpenAI: api_base = "https://YOUR-RESOURCE.openai.azure.com"

# Top-p sampling (0.0-1.0)
top_p = 1.0

# Frequency penalty (-2.0-2.0)
frequency_penalty = 0.0

# Presence penalty (-2.0-2.0)
presence_penalty = 0.0

# ------------------------------------------------------------------------------
# Ollama Configuration (Local server)
# ------------------------------------------------------------------------------
[llm.ollama]
# Model to use - must be pulled with 'ollama pull <model>'
# Available models: llama2, mistral, codellama, phi, neural-chat, etc.
model = "llama2"

# API endpoint - can also use OLLAMA_API_BASE env var
# api_base = "http://localhost:11434"

# Temperature
temperature = 0.7

# ------------------------------------------------------------------------------
# Local Model Configuration (Bumblebee/EMLX)
# ------------------------------------------------------------------------------
[llm.local]
# Model to use - HuggingFace model ID or local path
# Examples:
# - "microsoft/phi-2"
# - "meta-llama/Llama-2-7b-hf"
# - "mistralai/Mistral-7B-v0.1"
# - "/path/to/local/model"
model_path = "microsoft/phi-2"

# Device to use (auto-detected if not specified)
# Options: cpu, cuda, metal
# device = "cpu"

# Maximum tokens
max_tokens = 2048

# Temperature
temperature = 0.7

# ==============================================================================
# MCP Client Configuration
# ==============================================================================

[mcp]
# List of MCP servers to connect to

# ------------------------------------------------------------------------------
# Filesystem Server - Access local files
# ------------------------------------------------------------------------------
[[mcp.servers]]
name = "filesystem"
description = "Access local filesystem"
command = ["npx", "-y", "@modelcontextprotocol/server-filesystem", "$HOME"]
# Alternative with multiple directories:
# command = ["npx", "-y", "@modelcontextprotocol/server-filesystem", "/home/user", "/tmp", "/var/log"]

# Auto-connect on startup
auto_connect = true

# ------------------------------------------------------------------------------
# GitHub Server - Access GitHub repositories
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "github"
# description = "Access GitHub repositories"
# command = ["npx", "-y", "@modelcontextprotocol/server-github"]
# env = { GITHUB_TOKEN = "" }  # Or use GITHUB_TOKEN env var
# auto_connect = false

# ------------------------------------------------------------------------------
# PostgreSQL Server - Database access
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "postgres"
# description = "PostgreSQL database"
# command = ["npx", "-y", "@modelcontextprotocol/server-postgres"]
# env = { DATABASE_URL = "postgresql://user:password@localhost:5432/mydb" }

# ------------------------------------------------------------------------------
# SQLite Server - Local database access
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "sqlite"
# description = "SQLite database"
# command = ["npx", "-y", "@modelcontextprotocol/server-sqlite", "path/to/database.db"]

# ------------------------------------------------------------------------------
# Puppeteer Server - Browser automation
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "puppeteer"
# description = "Browser automation"
# command = ["npx", "-y", "@modelcontextprotocol/server-puppeteer"]

# ------------------------------------------------------------------------------
# Remote SSE Server - Connect to remote MCP server
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "remote-assistant"
# description = "Remote AI assistant"
# url = "http://api.example.com:8080"  # SSE endpoint
# # headers = { "Authorization" = "Bearer token123" }  # Optional auth

# ------------------------------------------------------------------------------
# Custom Local Server
# ------------------------------------------------------------------------------
# [[mcp.servers]]
# name = "my-tools"
# description = "Custom tools server"
# command = ["python", "/path/to/my-server.py"]
# working_dir = "/path/to/project"
# env = { 
#   CUSTOM_VAR = "value",
#   DEBUG = "true"
# }

# ==============================================================================
# MCP Server Mode Configuration
# ==============================================================================

[mcp_server]
# Enable stdio server mode (for other MCP clients to connect via stdio)
stdio_enabled = false

# Enable SSE server mode (for other MCP clients to connect via HTTP/SSE)
sse_enabled = false

# Port for SSE server
sse_port = 8080

# Host to bind SSE server (use 0.0.0.0 for all interfaces)
# sse_host = "127.0.0.1"

# ==============================================================================
# UI Configuration
# ==============================================================================

[ui]
# Color theme (dark, light, auto)
theme = "dark"

# Number of messages to keep in history
history_size = 1000

# Enable/disable Unicode emoji
emoji = true

# Enable streaming responses
streaming = true

# Show token counts
show_tokens = true

# Show cost estimates
show_cost = true

# Show timestamps on messages
show_timestamps = false

# Date format for timestamps
timestamp_format = "%H:%M:%S"

# ==============================================================================
# Context Management
# ==============================================================================

[context]
# Default maximum tokens for context window
max_tokens = 4096

# Reserved tokens for response
reserve_tokens = 500

# Default truncation strategy (sliding_window, smart)
strategy = "sliding_window"

# System prompt to always include
# system_prompt = "You are a helpful AI assistant."

# ==============================================================================
# Session Management
# ==============================================================================

[session]
# Auto-save sessions
auto_save = true

# Directory for saved sessions
save_directory = "~/.config/mcp_chat/sessions"

# Export format (markdown, json)
export_format = "markdown"

# ==============================================================================
# Command Aliases
# ==============================================================================

[aliases]
# Define custom command shortcuts
# status = ["/context", "/cost"]
# setup = ["/system You are a helpful assistant", "/tokens 8192"]
# morning = ["/clear", "Good morning! How can I help you today?"]

# ==============================================================================
# Logging and Debugging
# ==============================================================================

[debug]
# Log level (debug, info, warn, error)
log_level = "info"

# Log file path (empty to disable file logging)
# log_file = "~/.config/mcp_chat/logs/mcp_chat.log"

# Enable debug mode for MCP protocol
mcp_debug = false

# ==============================================================================
# Advanced Settings
# ==============================================================================

[advanced]
# Request timeout in seconds
request_timeout = 60

# Connection retry attempts
retry_attempts = 3

# Retry delay in seconds
retry_delay = 1

# Maximum concurrent MCP server connections
max_connections = 10