# MCP Chat Client Configuration Example

[llm]
# Default LLM backend to use
default = "anthropic"

[llm.anthropic]
# Get your API key from https://console.anthropic.com/
api_key = "YOUR_ANTHROPIC_API_KEY"
model = "claude-sonnet-4-20250514"
max_tokens = 4096

[llm.openai]
# Get your API key from https://platform.openai.com/api-keys
api_key = "YOUR_OPENAI_API_KEY"
model = "gpt-4"

[llm.local]
# Path to local model (for Bumblebee/Nx)
model_path = "models/llama-2-7b"
device = "cpu"  # or "cuda" if you have GPU

[[mcp.servers]]
# Example MCP server for filesystem access
name = "filesystem"
command = ["npx", "-y", "@modelcontextprotocol/server-filesystem", "/tmp"]

[[mcp.servers]]
# Example MCP server for GitHub (requires GITHUB_TOKEN)
name = "github"
command = ["npx", "-y", "@modelcontextprotocol/server-github"]
env = { GITHUB_TOKEN = "YOUR_GITHUB_TOKEN" }

# Uncomment to enable more servers:
# [[mcp.servers]]
# name = "brave-search"
# command = ["npx", "-y", "@modelcontextprotocol/server-brave-search"]
# env = { BRAVE_API_KEY = "YOUR_BRAVE_API_KEY" }

# [[mcp.servers]]
# name = "memory"
# command = ["npx", "-y", "@modelcontextprotocol/server-memory"]

[ui]
theme = "dark"
history_size = 1000
streaming = true

[context]
# Context window management settings
strategy = "smart"  # Options: "sliding_window", "smart"
max_tokens = 2048   # Maximum tokens to reserve for response